{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised classification combined with a segmentation\n",
    "\n",
    "Last edit: 18.10.2021     \n",
    "Authors: Yrneh Ulloa-Torrealba, Severin Herzsprung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set working directory and your relative paths for input and output data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(r\"C:\\Users\\ulloa-to\\git\\Advanced_Remote_sensing_HM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "\n",
    "src = \"W:\\Lehre\\object_based_classification\\python\"\n",
    "\n",
    "# paths to all input and output data folders\n",
    "geotiff_path = os.path.join(src, \"input_geotiff\\\\\")\n",
    "segmented_geotiff_path = os.path.join(src, \"segment_img\\\\\")\n",
    "vector_path = os.path.join(src, \"training_shp\\\\\")\n",
    "class_path = os.path.join(src, \"output_geotiff\\\\\")\n",
    "\n",
    "# paths to all input and output files\n",
    "geotiff = os.path.join(geotiff_path, \"aoi_02.tif\")\n",
    "segmented_geotiff = os.path.join(segmented_geotiff_path, \"segmented_raster.tif\")\n",
    "truth_shp = os.path.join(vector_path, \"truth_shp.shp\")\n",
    "train_shp = os.path.join(vector_path, \"train.shp\")\n",
    "test_shp = os.path.join(vector_path, \"test.shp\")\n",
    "class_tif = os.path.join(class_path, \"classified.tif\")\n",
    "\n",
    "# directory, where our logfile is saved:\n",
    "log_txt = os.path.join(src, 'log.txt')\n",
    "\n",
    "# variables for segmentation\n",
    "n_segments_var = 10000\n",
    "sigma_var = 0\n",
    "compactness_var = 0.5\n",
    "max_iter_var = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gdal\n",
    "from osgeo import ogr\n",
    "from skimage import exposure\n",
    "from skimage.segmentation import slic\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import scipy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bands 4 rows 313 columns 554\n"
     ]
    }
   ],
   "source": [
    "# read geotiff and extract relevant information\n",
    "\n",
    "driverTiff = gdal.GetDriverByName(\"GTiff\")\n",
    "geotiff_ds = gdal.Open(geotiff)\n",
    "nbands = geotiff_ds.RasterCount\n",
    "print(\"bands\", geotiff_ds.RasterCount, \"rows\", \n",
    "      geotiff_ds.RasterYSize, \"columns\", geotiff_ds.RasterXSize)\n",
    "\n",
    "# store shape info as an array\n",
    "band_data = []\n",
    "\n",
    "for i in range(1, nbands): # for 4 bands\n",
    "# for i in range(1, nbands+1): for the 5 bands\n",
    "    band = geotiff_ds.GetRasterBand(i).ReadAsArray()\n",
    "    band_data.append(band)\n",
    "\n",
    "# numpy.dstack: Stack arrays in sequence depth wise (concatenation along third axis).\n",
    "band_data = np.dstack(band_data)\n",
    "\n",
    "# exposure.rescale_intensity: Return image after stretching or shrinking its intensity levels.\n",
    "# scale image values from 0.0 - 1.0\n",
    "img = exposure.rescale_intensity(band_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\herzspru\\Anaconda3\\envs\\rio\\lib\\site-packages\\ipykernel_launcher.py:14: FutureWarning: skimage.measure.label's indexing starts from 0. In future version it will start from 1. To disable this warning, explicitely set the `start_label` parameter to 1.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# add start datetime to log \n",
    "now = datetime.datetime.now()\n",
    "print (\"\\n### Start date and time : \", now.strftime(\"%Y-%m-%d %H:%M:%S\"), file=open(log_txt, \"a\"))\n",
    "\n",
    "\n",
    "## perform segmentation\n",
    "\n",
    "print(\"# Segmentation Start\",file=open(log_txt, \"a\"))\n",
    "\n",
    "# start time measurement for the segmentation\n",
    "segmentation_start = time.time()\n",
    "\n",
    "# apply SLIC and extract (approximately) the supplied number of segments\n",
    "segments = slic(img, n_segments = n_segments_var, sigma = sigma_var,\n",
    "                compactness = compactness_var, max_iter = max_iter_var)\n",
    "\n",
    "# add segmentation time to log\n",
    "print(\"# Segmentation done in \", time.time() - segmentation_start, \"seconds\", file=open(log_txt, \"a\"))\n",
    "\n",
    "\n",
    "## extract metadata\n",
    "\n",
    "print(\"# Extraction of Metadata Start\",file=open(log_txt, \"a\"))\n",
    "\n",
    "# start time measurement for the extraction of Metadata\n",
    "metadata_start = time.time()\n",
    "\n",
    "# extract this information for the segments I will use for the classification, \n",
    "# after I choose the right parameters with the accuracy assessment\n",
    "def segment_features(segment_pixels):\n",
    "    features = []\n",
    "    npixels, nbands = segment_pixels.shape\n",
    "    for b in range(nbands):\n",
    "        stats = scipy.stats.describe(segment_pixels[:, b])\n",
    "        band_stats = list(stats.minmax) + list(stats)[2:]\n",
    "        if npixels == 1:\n",
    "            # in this case the variance will be nan and we don't want that\n",
    "            band_stats[3] = 0.0\n",
    "        features += band_stats\n",
    "    return features    \n",
    "\n",
    "# extract variables from each segment\n",
    "segment_ids = np.unique(segments)\n",
    "objects = []\n",
    "object_ids = []\n",
    "\n",
    "for id in segment_ids:\n",
    "    segment_pixels = img[segments == id]\n",
    "    object_features = segment_features(segment_pixels)\n",
    "    objects.append(object_features)\n",
    "    object_ids.append(id)\n",
    "\n",
    "# add metadata extraction time to log\n",
    "print(\"# Extraction of Metadata done in \", time.time() - metadata_start, \"seconds\", file=open(log_txt, \"a\"))\n",
    "\n",
    "\n",
    "## rasterize \n",
    "\n",
    "print(\"# Rasterization Start\",file=open(log_txt, \"a\"))\n",
    "\n",
    "# start time measurement for rasterization\n",
    "rasterize_start = time.time()\n",
    "\n",
    "# save segments to raster\n",
    "output_fullpath = segmented_geotiff\n",
    "segments_ds = driverTiff.Create(output_fullpath, geotiff_ds.RasterXSize,\n",
    "                                geotiff_ds.RasterYSize, 1, gdal.GDT_Float32)\n",
    "segments_ds.SetGeoTransform(geotiff_ds.GetGeoTransform())\n",
    "segments_ds.SetProjection(geotiff_ds.GetProjectionRef())\n",
    "segments_ds.GetRasterBand(1).WriteArray(segments)\n",
    "segments_ds = None\n",
    "\n",
    "# rasterize the training data\n",
    "\n",
    "# read shapefile to geopandas geodataframe\n",
    "gdf = gpd.read_file(truth_shp)\n",
    "\n",
    "# get names of land cover classes/labels\n",
    "class_names = gdf['label'].unique()\n",
    "print('class names: ', class_names, file=open(log_txt, \"a\"))\n",
    "\n",
    "# create a unique id (integer) for each land cover class/label\n",
    "class_ids = np.arange(class_names.size) + 1\n",
    "print('class ids: ', class_ids, file=open(log_txt, \"a\"))\n",
    "\n",
    "# add a new column to geodatafame with the id for each class/label\n",
    "gdf['id'] = gdf['label'].map(dict(zip(class_names, class_ids)))\n",
    "\n",
    "# split the truth data into training and test data sets and save each to a new shapefile\n",
    "gdf_train = gdf.sample(frac=0.7)\n",
    "gdf_test = gdf.drop(gdf_train.index)\n",
    "print('truth data:', gdf.shape, '   train data:', gdf_train.shape,\n",
    "      '   test data:', gdf_test.shape, file=open(log_txt, \"a\"))\n",
    "gdf_train.to_file(train_shp)\n",
    "gdf_test.to_file(test_shp)\n",
    "\n",
    "# open the points file to use for training data\n",
    "train_fn = train_shp\n",
    "train_ds = ogr.Open(train_fn)\n",
    "lyr = train_ds.GetLayer()\n",
    "\n",
    "# create a new raster layer in memory\n",
    "driver = gdal.GetDriverByName('MEM')\n",
    "target_ds = driver.Create('', geotiff_ds.RasterXSize, geotiff_ds.RasterYSize, 1, gdal.GDT_UInt16)\n",
    "target_ds.SetGeoTransform(geotiff_ds.GetGeoTransform())\n",
    "target_ds.SetProjection(geotiff_ds.GetProjection())\n",
    "\n",
    "# rasterize the training points\n",
    "options = ['ATTRIBUTE=id']\n",
    "gdal.RasterizeLayer(target_ds, [1], lyr, options=options)\n",
    "\n",
    "# add rasterization time to log\n",
    "print(\"# Rasterization done in \", time.time() - rasterize_start, \"seconds\", file=open(log_txt, \"a\"))\n",
    "\n",
    "\n",
    "## classify\n",
    "\n",
    "print(\"# Classification Start\",file=open(log_txt, \"a\"))\n",
    "\n",
    "# start time measurement for classification\n",
    "classify_start = time.time()\n",
    "\n",
    "ground_truth = target_ds.GetRasterBand(1).ReadAsArray()\n",
    "classes = np.unique(ground_truth)[1:]\n",
    "\n",
    "# Get segments representing each land cover classification type and ensure no segment represents more than one class.\n",
    "segments_per_class = {}\n",
    "for klass in classes:\n",
    "    segments_of_class = segments[ground_truth == klass]\n",
    "    segments_per_class[klass] = set(segments_of_class)\n",
    "    print(\"Training segments for class\", klass, \":\", len(segments_of_class), file=open(log_txt, \"a\"))\n",
    "\n",
    "intersection = set()\n",
    "accum = set()\n",
    " \n",
    "for class_segments in segments_per_class.values():\n",
    "    intersection |= accum.intersection(class_segments)\n",
    "    accum |= class_segments\n",
    "assert len(intersection) == 0, \"Segment(s) represent multiple classes\"\n",
    "\n",
    "# Classify the image\n",
    "train_img = np.copy(segments)\n",
    "threshold = train_img.max() + 1\n",
    " \n",
    "for klass in classes:\n",
    "    class_label = threshold + klass\n",
    "    for segment_id in segments_per_class[klass]:\n",
    "        train_img[train_img == segment_id] = class_label\n",
    "\n",
    "train_img[train_img <= threshold] = 0\n",
    "train_img[train_img > threshold] -= threshold\n",
    " \n",
    "training_objects = []\n",
    "training_labels = []\n",
    " \n",
    "for klass in classes:\n",
    "    class_train_object = [v for i, v in enumerate(objects) if segment_ids[i] in segments_per_class[klass]]\n",
    "    training_labels += [klass] * len(class_train_object)\n",
    "    training_objects += class_train_object\n",
    "    print('Training objects for class', klass, ':', len(class_train_object), file=open(log_txt, \"a\"))\n",
    "\n",
    "# fit Random Forest classifier\n",
    "classifier = RandomForestClassifier(n_jobs=-1)\n",
    "classifier.fit(training_objects, training_labels)\n",
    "\n",
    "# predict classifications\n",
    "predicted = classifier.predict(objects)\n",
    "\n",
    "# apply prediction to numpy array\n",
    "clf = np.copy(segments)\n",
    "for segment_id, klass in zip(segment_ids, predicted):\n",
    "    clf[clf == segment_id] = klass\n",
    "\n",
    "mask = np.sum(img, axis=2)\n",
    "mask[mask > 0.0] = 1.0\n",
    "mask[mask == 0.0] = -1.0\n",
    "clf = np.multiply(clf, mask)\n",
    "clf[clf < 0] = -9999.0\n",
    " \n",
    "#Saving classificaiton to raster with gdal\n",
    "clfds = driverTiff.Create(class_tif, geotiff_ds.RasterXSize, geotiff_ds.RasterYSize,\n",
    "                          1, gdal.GDT_Float32)\n",
    "clfds.SetGeoTransform(geotiff_ds.GetGeoTransform())\n",
    "clfds.SetProjection(geotiff_ds.GetProjection())\n",
    "clfds.GetRasterBand(1).SetNoDataValue(-9999.0)\n",
    "clfds.GetRasterBand(1).WriteArray(clf)\n",
    "clfds = None\n",
    " \n",
    "# add classification time to log\n",
    "print(\"# Classification done in \", time.time() - rasterize_start, \"seconds\", file=open(log_txt, \"a\"))\n",
    "\n",
    "\n",
    "## accuracy assessment\n",
    "\n",
    "print(\"# Accuracy assessment Start\",file=open(log_txt, \"a\"))\n",
    "\n",
    "# start time measurement for the accuracy assessment\n",
    "accu_start = time.time()\n",
    "\n",
    "# open the points file to use for test data\n",
    "test_fn = test_shp\n",
    "test_ds = ogr.Open(test_fn)\n",
    "lyr = test_ds.GetLayer()\n",
    "\n",
    "# create a new raster layer in memory\n",
    "driver = gdal.GetDriverByName('MEM')\n",
    "target_ds = driver.Create('', geotiff_ds.RasterXSize, geotiff_ds.RasterYSize, 1, gdal.GDT_UInt16)\n",
    "target_ds.SetGeoTransform(geotiff_ds.GetGeoTransform())\n",
    "target_ds.SetProjection(geotiff_ds.GetProjection())\n",
    "\n",
    "# rasterize the test points\n",
    "options = ['ATTRIBUTE=id']\n",
    "gdal.RasterizeLayer(target_ds, [1], lyr, options=options)\n",
    "\n",
    "# set test data as truth\n",
    "truth = target_ds.GetRasterBand(1).ReadAsArray()\n",
    "\n",
    "# open classified image and set as prediction\n",
    "pred_ds = gdal.Open(class_tif)\n",
    "pred = pred_ds.GetRasterBand(1).ReadAsArray()\n",
    "\n",
    "idx = np.nonzero(truth)\n",
    "\n",
    "# create confusion matrix\n",
    "cm = metrics.confusion_matrix(truth[idx], pred[idx])\n",
    " \n",
    "# pixel accuracy\n",
    "print(\"Confusion matrix: \",'\\n',cm, file=open(log_txt, \"a\"))\n",
    " \n",
    "print(\"Diagonal: \",cm.diagonal(), file=open(log_txt, \"a\"))\n",
    "print(\"Sum: \",cm.sum(axis=0), file=open(log_txt, \"a\"))\n",
    " \n",
    "accuracy = cm.diagonal() / cm.sum(axis=0)\n",
    "print(\"Accuracy: \",accuracy, file=open(log_txt, \"a\"))\n",
    "\n",
    "# add accuracy assessment time to log\n",
    "print(\"# Accuracy assessment done in \", time.time() - accu_start, \"seconds\", file=open(log_txt, \"a\"))\n",
    "\n",
    "# add end datetime to log\n",
    "now = datetime.datetime.now()\n",
    "print (\"### End date and time: \", now.strftime(\"%Y-%m-%d %H:%M:%S\"), file=open(log_txt, \"a\"))\n",
    "\n",
    "print(\"Done\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
