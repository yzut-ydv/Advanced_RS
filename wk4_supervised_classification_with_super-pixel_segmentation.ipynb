{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised classification combined with a super-pixel segmentation\n",
    "\n",
    "Last edit: 18.10.2021     \n",
    "Authors: Yrneh Ulloa-Torrealba, Severin Herzsprung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Set working directory and your relative paths for input and output data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(r\"C:\\Users\\ulloa-to\\git\\Advanced_Remote_sensing_HM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths to all input and output data folders\n",
    "geotiff_path = \"data/s2_change_rasters/\"\n",
    "vector_path = \"data/vector/\"\n",
    "output_path = \"data/wk4_results/\"\n",
    "\n",
    "# paths to all input files\n",
    "geotiff = os.path.join(geotiff_path, \"july_32N_subset.tif\")\n",
    "truth_shp = os.path.join(vector_path, \"wk4_truth.shp\")\n",
    "train_shp = os.path.join(vector_path, \"wk4_train.shp\")\n",
    "test_shp = os.path.join(vector_path, \"wk4_test.shp\")\n",
    "\n",
    "# paths to all output files\n",
    "segmented_geotiff = os.path.join(output_path, \"wk4_segmented_raster.tif\")\n",
    "class_tif = os.path.join(output_path, \"wk4_classified.tif\")\n",
    "\n",
    "# directory, where our logfile is saved:\n",
    "log_txt = os.path.join(output_path, 'wk4_log.txt')\n",
    "\n",
    "# variables for segmentation\n",
    "n_segments_var = 15000\n",
    "compactness_var = 0.5\n",
    "max_iter_var = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Load the information of the raster to be classified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input raster has: 4 bands 306 rows, and 547 columns\n"
     ]
    }
   ],
   "source": [
    "import gdal\n",
    "\n",
    "# read geotiff and extract relevant information\n",
    "driverTiff = gdal.GetDriverByName(\"GTiff\")\n",
    "geotiff_ds = gdal.Open(geotiff)\n",
    "nbands = geotiff_ds.RasterCount\n",
    "print(\"Input raster has:\", geotiff_ds.RasterCount, \n",
    "      \"bands\", geotiff_ds.RasterYSize, \n",
    "      \"rows, and\", geotiff_ds.RasterXSize, \"columns\")\n",
    "\n",
    "# store shape info as an array\n",
    "band_data = []\n",
    "\n",
    "for i in range(1, nbands): # for 4 bands\n",
    "# for i in range(1, nbands+1): for the 5 bands\n",
    "    band = geotiff_ds.GetRasterBand(i).ReadAsArray()\n",
    "    band_data.append(band)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.  Store the raster band information in an array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "    \n",
    "# numpy.dstack: Stack arrays in sequence depth wise (concatenation along third axis).\n",
    "band_data = np.dstack(band_data)\n",
    "\n",
    "from skimage import exposure\n",
    "\n",
    "# exposure.rescale_intensity: Return image after stretching or shrinking its intensity levels.\n",
    "# scale image values from 0.0 - 1.0\n",
    "img = exposure.rescale_intensity(band_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Perform the SLIC segmentation. Store the process in a TXT file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import time\n",
    "\n",
    "# add start datetime to log \n",
    "now = datetime.datetime.now()\n",
    "print (\"\\n### Start date and time : \", now.strftime(\"%Y-%m-%d %H:%M:%S\"), file=open(log_txt, \"a\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.segmentation import slic\n",
    "\n",
    "# Perform segmentation\n",
    "\n",
    "print(\"# Segmentation Start\",file=open(log_txt, \"a\"))\n",
    "\n",
    "# start time measurement for the segmentation\n",
    "segmentation_start = time.time()\n",
    "\n",
    "# apply SLIC and extract (approximately) the supplied number of segments\n",
    "segments = slic(img, n_segments = n_segments_var,\n",
    "                compactness = compactness_var, max_iter = max_iter_var, start_label=0)\n",
    "\n",
    "# add segmentation time to log\n",
    "print(\"# Segmentation done in \", time.time() - segmentation_start, \"seconds\", file=open(log_txt, \"a\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. After creating the segments, extract pixel information from the whole segment. The function **scipy.stats.describe** allows to calculate the following metrics:    \n",
    "* number of observations   \n",
    "* minmax\n",
    "* mean   \n",
    "* variance  \n",
    "* skewness  \n",
    "* kurtosis\n",
    "\n",
    "With the statement `band_stats = list(stats.minmax) + list(stats)[2:]`, the following were selected:   \n",
    "* minmax\n",
    "* mean   \n",
    "* variance  \n",
    "* skewness  \n",
    "* kurtosis\n",
    "\n",
    "To understand the second argument, use the following reproducible example:    \n",
    "`a = [0, 1, 2, 3, 4, 5]    \n",
    "a[2:]\n",
    "`\n",
    "\n",
    "[Click for more info](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.describe.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "# Extract metadata\n",
    "\n",
    "print(\"# Extraction of Metadata Start\",file=open(log_txt, \"a\"))\n",
    "\n",
    "# start time measurement for the extraction of Metadata\n",
    "metadata_start = time.time()\n",
    "\n",
    "# extract this information for the segments I will use for the classification, \n",
    "# after I choose the right parameters with the accuracy assessment\n",
    "def segment_features(segment_pixels):\n",
    "    features = []\n",
    "    npixels, nbands = segment_pixels.shape\n",
    "    for b in range(nbands):\n",
    "        stats = scipy.stats.describe(segment_pixels[:, b])\n",
    "        band_stats = list(stats.minmax) + list(stats)[2:]\n",
    "        if npixels == 1:\n",
    "            # in this case the variance will be nan and we don't want that\n",
    "            band_stats[3] = 0.0\n",
    "        features += band_stats\n",
    "    return features\n",
    "\n",
    "# extract variables from each segment\n",
    "segment_ids = np.unique(segments)\n",
    "objects = []\n",
    "object_ids = []\n",
    "\n",
    "for id in segment_ids:\n",
    "    segment_pixels = img[segments == id]\n",
    "    object_features = segment_features(segment_pixels)\n",
    "    objects.append(object_features)\n",
    "    object_ids.append(id)\n",
    "\n",
    "# add metadata extraction time to log\n",
    "print(\"# Extraction of Metadata done in \", time.time() - metadata_start, \"seconds\", file=open(log_txt, \"a\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Rasterize the segments with the metrics calculated. You can export this file and have a look at it in a software like QGIS. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## rasterize the segments\n",
    "print(\"# Rasterization Start\",file=open(log_txt, \"a\"))\n",
    "\n",
    "# start time measurement for rasterization\n",
    "rasterize_start = time.time()\n",
    "\n",
    "# save segments to raster\n",
    "output_fullpath = segmented_geotiff\n",
    "segments_ds = driverTiff.Create(output_fullpath, geotiff_ds.RasterXSize,\n",
    "                                geotiff_ds.RasterYSize, 1, gdal.GDT_Float32)\n",
    "segments_ds.SetGeoTransform(geotiff_ds.GetGeoTransform())\n",
    "segments_ds.SetProjection(geotiff_ds.GetProjectionRef())\n",
    "segments_ds.GetRasterBand(1).WriteArray(segments)\n",
    "segments_ds = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Load the vector truth data and rasterize it as well. Afterwards, split the truth data into 70% train and 30% test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "# rasterize the training data\n",
    "\n",
    "# read shapefile to geopandas geodataframe\n",
    "gdf = gpd.read_file(truth_shp)\n",
    "\n",
    "# get names of land cover classes/labels\n",
    "class_names = gdf['label'].unique()\n",
    "print('class names: ', class_names, file=open(log_txt, \"a\"))\n",
    "\n",
    "# create a unique id (integer) for each land cover class/label\n",
    "class_ids = np.arange(class_names.size) + 1\n",
    "print('class ids: ', class_ids, file=open(log_txt, \"a\"))\n",
    "\n",
    "# add a new column to geodatafame with the id for each class/label\n",
    "gdf['id'] = gdf['label'].map(dict(zip(class_names, class_ids)))\n",
    "\n",
    "# split the truth data into training and test data sets and save each to a new shapefile\n",
    "gdf_train = gdf.sample(frac=0.7) # 70% train\n",
    "gdf_test = gdf.drop(gdf_train.index) # 30% test\n",
    "print('truth data:', gdf.shape, '   train data:', gdf_train.shape,\n",
    "      'test data:', gdf_test.shape, file=open(log_txt, \"a\"))\n",
    "\n",
    "# export train and test as shapefiles\n",
    "gdf_train.to_file(train_shp)\n",
    "gdf_test.to_file(test_shp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Load the training data with the OGR library. Rasterize the training data but do not export this file. Keep it only in memory for short use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from osgeo import ogr\n",
    "\n",
    "# open the points file to use for training data\n",
    "train_fn = train_shp\n",
    "train_ds = ogr.Open(train_fn)\n",
    "lyr = train_ds.GetLayer()\n",
    "\n",
    "# create a new raster layer in memory\n",
    "driver = gdal.GetDriverByName('MEM')\n",
    "target_ds = driver.Create('', geotiff_ds.RasterXSize, geotiff_ds.RasterYSize, 1, gdal.GDT_UInt16)\n",
    "target_ds.SetGeoTransform(geotiff_ds.GetGeoTransform())\n",
    "target_ds.SetProjection(geotiff_ds.GetProjection())\n",
    "\n",
    "# rasterize the training points\n",
    "options = ['ATTRIBUTE=id']\n",
    "gdal.RasterizeLayer(target_ds, [1], lyr, options=options)\n",
    "\n",
    "# add rasterization time to log\n",
    "print(\"# Rasterization done in \", time.time() - rasterize_start, \"seconds\", file=open(log_txt, \"a\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Perform a supervised classification using Random Forest.   \n",
    "Input data are the rasterized segments with metrics, and the rasterized training data in a temporary layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "## classify\n",
    "\n",
    "print(\"# Classification Start\",file=open(log_txt, \"a\"))\n",
    "\n",
    "# start time measurement for classification\n",
    "classify_start = time.time()\n",
    "\n",
    "ground_truth = target_ds.GetRasterBand(1).ReadAsArray()\n",
    "classes = np.unique(ground_truth)[1:]\n",
    "\n",
    "# Get segments representing each land cover classification type and ensure no segment represents more than one class.\n",
    "segments_per_class = {}\n",
    "for klass in classes:\n",
    "    segments_of_class = segments[ground_truth == klass]\n",
    "    segments_per_class[klass] = set(segments_of_class)\n",
    "    print(\"Training segments for class\", klass, \":\", len(segments_of_class), file=open(log_txt, \"a\"))\n",
    "\n",
    "# intersection = set()\n",
    "# accum = set()\n",
    " \n",
    "# for class_segments in segments_per_class.values():\n",
    "#     intersection |= accum.intersection(class_segments)\n",
    "#     accum |= class_segments\n",
    "# assert len(intersection) == 0, \"Segment(s) represent multiple classes\"\n",
    "\n",
    "# Classify the image\n",
    "train_img = np.copy(segments)\n",
    "threshold = train_img.max() + 1\n",
    " \n",
    "for klass in classes:\n",
    "    class_label = threshold + klass\n",
    "    for segment_id in segments_per_class[klass]:\n",
    "        train_img[train_img == segment_id] = class_label\n",
    "\n",
    "train_img[train_img <= threshold] = 0\n",
    "train_img[train_img > threshold] -= threshold\n",
    " \n",
    "training_objects = []\n",
    "training_labels = []\n",
    " \n",
    "for klass in classes:\n",
    "    class_train_object = [v for i, v in enumerate(objects) if segment_ids[i] in segments_per_class[klass]]\n",
    "    training_labels += [klass] * len(class_train_object)\n",
    "    training_objects += class_train_object\n",
    "    print('Training objects for class', klass, ':', len(class_train_object), file=open(log_txt, \"a\"))\n",
    "\n",
    "# fit Random Forest classifier\n",
    "classifier = RandomForestClassifier(n_jobs=-1)\n",
    "classifier.fit(training_objects, training_labels)\n",
    "\n",
    "# predict classifications\n",
    "predicted = classifier.predict(objects)\n",
    "\n",
    "# apply prediction to numpy array\n",
    "clf = np.copy(segments)\n",
    "for segment_id, klass in zip(segment_ids, predicted):\n",
    "    clf[clf == segment_id] = klass\n",
    "\n",
    "mask = np.sum(img, axis=2)\n",
    "mask[mask > 0.0] = 1.0\n",
    "mask[mask == 0.0] = -1.0\n",
    "clf = np.multiply(clf, mask)\n",
    "clf[clf < 0] = -9999.0\n",
    " \n",
    "#Saving classification to raster with gdal\n",
    "clfds = driverTiff.Create(class_tif, geotiff_ds.RasterXSize, geotiff_ds.RasterYSize,\n",
    "                          1, gdal.GDT_Float32)\n",
    "clfds.SetGeoTransform(geotiff_ds.GetGeoTransform())\n",
    "clfds.SetProjection(geotiff_ds.GetProjection())\n",
    "clfds.GetRasterBand(1).SetNoDataValue(-9999.0)\n",
    "clfds.GetRasterBand(1).WriteArray(clf)\n",
    "clfds = None\n",
    " \n",
    "# add classification time to log\n",
    "print(\"# Classification done in \", time.time() - rasterize_start, \"seconds\", file=open(log_txt, \"a\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Perform the accuracy assessment. For this step, load your test SHP as a temporary raster layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## accuracy assessment\n",
    "\n",
    "print(\"# Accuracy assessment Start\",file=open(log_txt, \"a\"))\n",
    "\n",
    "# start time measurement for the accuracy assessment\n",
    "accu_start = time.time()\n",
    "\n",
    "# open the points file to use for test data\n",
    "test_fn = test_shp\n",
    "test_ds = ogr.Open(test_fn)\n",
    "lyr = test_ds.GetLayer()\n",
    "\n",
    "# create a new raster layer in memory\n",
    "driver = gdal.GetDriverByName('MEM')\n",
    "target_ds = driver.Create('', geotiff_ds.RasterXSize, geotiff_ds.RasterYSize, 1, gdal.GDT_UInt16)\n",
    "target_ds.SetGeoTransform(geotiff_ds.GetGeoTransform())\n",
    "target_ds.SetProjection(geotiff_ds.GetProjection())\n",
    "\n",
    "# rasterize the test points\n",
    "options = ['ATTRIBUTE=id']\n",
    "gdal.RasterizeLayer(target_ds, [1], lyr, options=options)\n",
    "\n",
    "# set test data as truth\n",
    "truth = target_ds.GetRasterBand(1).ReadAsArray()\n",
    "\n",
    "# open classified image and set as prediction\n",
    "pred_ds = gdal.Open(class_tif)\n",
    "pred = pred_ds.GetRasterBand(1).ReadAsArray()\n",
    "\n",
    "idx = np.nonzero(truth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "# create confusion matrix\n",
    "cm = metrics.confusion_matrix(truth[idx], pred[idx])\n",
    " \n",
    "# pixel accuracy\n",
    "print(\"Confusion matrix: \",'\\n',cm, file=open(log_txt, \"a\"))\n",
    " \n",
    "print(\"Diagonal: \",cm.diagonal(), file=open(log_txt, \"a\"))\n",
    "print(\"Sum: \",cm.sum(axis=0), file=open(log_txt, \"a\"))\n",
    " \n",
    "accuracy = cm.diagonal() / cm.sum(axis=0)\n",
    "print(\"Accuracy: \",accuracy, file=open(log_txt, \"a\"))\n",
    "\n",
    "# add accuracy assessment time to log\n",
    "print(\"# Accuracy assessment done in \", time.time() - accu_start, \"seconds\", file=open(log_txt, \"a\"))\n",
    "\n",
    "# add end datetime to log\n",
    "now = datetime.datetime.now()\n",
    "print (\"### End date and time: \", now.strftime(\"%Y-%m-%d %H:%M:%S\"), file=open(log_txt, \"a\"))\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are done! Check the log file and the output folder to see your results. Adapt all the necessary parameters and run again the classification in order to improve your accuracy.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
